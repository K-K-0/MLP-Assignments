{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['created_date'] = pd.to_datetime(train['created_date'])\n",
    "train['created_date'].dt.month_name().str.lower().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246707da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['total_emotion'] = train['emoticon_1'] + train['emoticon_2'] + train['emoticon_3']\n",
    "\n",
    "train['total_emotion'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment'] = train['comment'].fillna('')\n",
    "label = train[train['label'] == 3]\n",
    "medium_len = label['comment'].str.len().median()\n",
    "medium_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = train['upvote'].min()\n",
    "max_value = train['upvote'].max()\n",
    "\n",
    "min_max = (10 - min_value) / (max_value - min_value)\n",
    "min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label1 = train[train['label'] == 1]\n",
    "word_count = label1['comment'].fillna('').apply(lambda x: len(x.split()))\n",
    "word_count.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee21f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment'].str.contains('trump', case=False, na=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "text = train.loc[0, 'comment']\n",
    "text = text.lower()\n",
    "\n",
    "text = text.translate(str.maketrans('','', string.punctuation))\n",
    "\n",
    "stop_words =  ['a', 'an', 'the', 'and', 'or', 'but', 'if', 'because', 'as', 'of', 'at', 'by', 'for', 'with', 'about', 'to', 'from', 'up', 'on', 'in', 'out', 'over', 'under', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'it', 'its', 'they', 'them', 'their', 'she', 'her', 'he', 'him', 'his', 'this', 'that', 'which', 'who', 'whom', 'i', 'me', 'my', 'we', 'our', 'you', 'your'] \n",
    "\n",
    "words = text.split()\n",
    "\n",
    "filter_words = [w for w in words if w not in stop_words]\n",
    "\n",
    "len(filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train['comment'].fillna('').str.lower().str.strip()\n",
    "unique_token = set([token for sublist in text for token in sublist])\n",
    "\n",
    "len(unique_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    min_df=5,\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(train['comment'].fillna(''))\n",
    "\n",
    "len(vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
